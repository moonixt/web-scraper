# ğŸ•·ï¸ Katana + Scrapy Integration

Uma integraÃ§Ã£o poderosa entre **Katana** (descoberta de URLs) e **Scrapy** (web scraping) para coleta automatizada de conteÃºdo web e imagens.

## ğŸ“‹ Ãndice

- [InstalaÃ§Ã£o](#instalaÃ§Ã£o)
- [ConfiguraÃ§Ã£o](#configuraÃ§Ã£o)
- [Uso BÃ¡sico](#uso-bÃ¡sico)
- [Comandos Ãšteis](#comandos-Ãºteis)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [ConfiguraÃ§Ãµes AvanÃ§adas](#configuraÃ§Ãµes-avanÃ§adas)
- [Exemplos de Uso](#exemplos-de-uso)
- [Troubleshooting](#troubleshooting)

## ğŸš€ InstalaÃ§Ã£o

### DependÃªncias Principais

```bash
# Scrapy e dependÃªncias
pip install scrapy beautifulsoup4 Pillow

# Katana (ProjectDiscovery)
go install github.com/projectdiscovery/katana/cmd/katana@latest

# Ou usando conda
conda install -c conda-forge scrapy beautifulsoup4 pillow
```

### Verificar InstalaÃ§Ã£o

```bash
# Verificar Scrapy
scrapy version

# Verificar Katana
katana -version

# Verificar Pillow
python -c "from PIL import Image; print('Pillow OK')"
```

## âš™ï¸ ConfiguraÃ§Ã£o

### 1. ConfiguraÃ§Ã£o do Scrapy (`settings.py`)

```python
# Pipeline de imagens
ITEM_PIPELINES = {
    'scrapy.pipelines.images.ImagesPipeline': 1,
}

# ConfiguraÃ§Ãµes de imagens
IMAGES_STORE = 'images'
IMAGES_MIN_HEIGHT = 50
IMAGES_MIN_WIDTH = 50

# Outras configuraÃ§Ãµes Ãºteis
ROBOTSTXT_OBEY = True
DOWNLOAD_DELAY = 1
RANDOMIZE_DOWNLOAD_DELAY = True
```

### 2. Estrutura de Items (`items.py`)

```python
import scrapy

class PageItem(scrapy.Item):
    title = scrapy.Field()
    url = scrapy.Field()
    content = scrapy.Field()
    image_urls = scrapy.Field()
    images = scrapy.Field()
```

## ğŸ¯ Uso BÃ¡sico

### 1. Gerar URLs com Katana

```bash
# Descoberta bÃ¡sica de URLs
katana -u https://exemplo.com -o urls.jsonl

# Descoberta com profundidade
katana -u https://exemplo.com -d 3 -o urls_deep.jsonl

# Descoberta com filtros
katana -u https://exemplo.com -f qurl -o urls_filtered.jsonl

# MÃºltiplos domÃ­nios
katana -list targets.txt -o urls_multi.jsonl
```

### 2. Executar Spider com Katana

```bash
# Comando bÃ¡sico
scrapy crawl page -a katana_file="urls.jsonl"

# Especificar spider especÃ­fico
scrapy crawl page -a katana_file="urls.jsonl" -s SPIDER_MODULES=tutorial.spiders.crawlocal

# Com configuraÃ§Ãµes customizadas
scrapy crawl page -a katana_file="urls.jsonl" -s IMAGES_STORE=custom_images -s DOWNLOAD_DELAY=2
```

## ğŸ“š Comandos Ãšteis

### Comandos Katana

```bash
# Descoberta bÃ¡sica
katana -u https://site.com -o output.jsonl

# Com maior profundidade
katana -u https://site.com -d 5 -o deep_crawl.jsonl

# Apenas subdomÃ­nios
katana -u https://site.com -field-scope subs -o subdomains.jsonl

# Com rate limiting
katana -u https://site.com -rl 100 -o rate_limited.jsonl

# Salvar apenas URLs Ãºnicas
katana -u https://site.com -f qurl -unique -o unique_urls.jsonl

# Descoberta com JavaScript rendering
katana -u https://site.com -js-crawl -o js_rendered.jsonl

# Filtrar por extensÃµes
katana -u https://site.com -ef png,jpg,gif -o no_images.jsonl

# Timeout customizado
katana -u https://site.com -timeout 30 -o timeout_30s.jsonl
```

### Comandos Scrapy

```bash
# Listar spiders disponÃ­veis
scrapy list

# Executar com log detalhado
scrapy crawl page -a katana_file="urls.jsonl" -L DEBUG

# Salvar logs em arquivo
scrapy crawl page -a katana_file="urls.jsonl" -L INFO -s LOG_FILE=scrapy.log

# Executar em background
nohup scrapy crawl page -a katana_file="urls.jsonl" > output.log 2>&1 &

# Parar execuÃ§Ã£o apÃ³s X itens
scrapy crawl page -a katana_file="urls.jsonl" -s CLOSESPIDER_ITEMCOUNT=100

# Configurar User-Agent
scrapy crawl page -a katana_file="urls.jsonl" -s USER_AGENT="Custom Bot 1.0"

# Desabilitar robots.txt
scrapy crawl page -a katana_file="urls.jsonl" -s ROBOTSTXT_OBEY=False

# Configurar proxy
scrapy crawl page -a katana_file="urls.jsonl" -s HTTP_PROXY=http://proxy:8080
```

### Comandos de AnÃ¡lise

```bash
# Contar URLs no arquivo Katana
wc -l urls.jsonl

# Ver primeiras URLs
head -5 urls.jsonl

# Extrair apenas URLs (formato Katana)
cat urls.jsonl | jq -r '.request.endpoint' | head -10

# Contar domÃ­nios Ãºnicos
cat urls.jsonl | jq -r '.request.endpoint' | cut -d'/' -f3 | sort | uniq -c

# Verificar arquivos gerados
find . -name "scraped_*" -type d

# Contar arquivos HTML gerados
find . -name "*.html" | wc -l

# Verificar estrutura de imagens organizadas
find images/ -type f | sort

# Ver imagens por arquivo JSON especÃ­fico
ls images/test/
ls images/test_simple/

# Contar imagens por pasta
find images/ -name "*.jpg" -o -name "*.png" | cut -d'/' -f2 | sort | uniq -c

# Tamanho de imagens por pasta
du -sh images/*/

# EstatÃ­sticas de execuÃ§Ã£o
grep "item_scraped_count\|file_count" scrapy.log
```

### ğŸ“Š Comandos de OrganizaÃ§Ã£o

```bash
# Ver estrutura completa organizada
find . -name "scraped_*" -o -name "images" | sort

# Contar arquivos por projeto
for dir in scraped_*; do echo "$dir: $(ls $dir | wc -l) arquivos"; done

# Verificar imagens por arquivo JSON
for dir in images/*/; do echo "$(basename $dir): $(ls $dir | wc -l) imagens"; done

# Limpar execuÃ§Ãµes antigas (cuidado!)
rm -rf scraped_* images/*/

# Backup de resultados por data
tar -czf backup_$(date +%Y%m%d).tar.gz scraped_* images/
```

## ğŸ“ Estrutura do Projeto

```
tutorial/
â”œâ”€â”€ scrapy.cfg              # ConfiguraÃ§Ã£o do projeto Scrapy
â”œâ”€â”€ tutorial/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ items.py            # DefiniÃ§Ã£o dos items
â”‚   â”œâ”€â”€ middlewares.py      # Middlewares customizados
â”‚   â”œâ”€â”€ pipelines.py        # Pipeline customizado de imagens
â”‚   â”œâ”€â”€ settings.py         # ConfiguraÃ§Ãµes do Scrapy
â”‚   â””â”€â”€ spiders/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ crawlocal.py    # Spider principal
â”‚       â”œâ”€â”€ test.jsonl      # Arquivo Katana (formato completo)
â”‚       â”œâ”€â”€ test_simple.jsonl # Arquivo simplificado
â”‚       â””â”€â”€ teste_final.jsonl # Exemplo de teste
â”œâ”€â”€ images/                 # Imagens baixadas (organizadas por arquivo JSON)
â”‚   â”œâ”€â”€ test/               # Imagens do arquivo test.jsonl
â”‚   â”‚   â”œâ”€â”€ [hash].jpg
â”‚   â”‚   â””â”€â”€ [hash].png
â”‚   â”œâ”€â”€ test_simple/        # Imagens do arquivo test_simple.jsonl
â”‚   â”‚   â”œâ”€â”€ [hash].jpg
â”‚   â”‚   â””â”€â”€ [hash].png
â”‚   â””â”€â”€ teste_final/        # Imagens do arquivo teste_final.jsonl
â”‚       â”œâ”€â”€ [hash].jpg
â”‚       â””â”€â”€ [hash].png
â”œâ”€â”€ scraped_[filename]/     # ConteÃºdo extraÃ­do (organizado por arquivo JSON)
â”‚   â”œâ”€â”€ page-[url].html     # HTML original
â”‚   â””â”€â”€ pretty-[url].html   # HTML formatado
â””â”€â”€ README.md
```

### ğŸ—‚ï¸ OrganizaÃ§Ã£o AutomÃ¡tica

O sistema agora organiza automaticamente tanto **imagens** quanto **conteÃºdo** baseado no nome do arquivo JSON de entrada:

- **Imagens**: `images/[nome_arquivo]/[hash].[ext]`
- **ConteÃºdo**: `scraped_[nome_arquivo]/[arquivos_html]`

**Exemplo:**
```bash
# Para arquivo test.jsonl
images/test/          # Imagens
scraped_test/         # ConteÃºdo HTML

# Para arquivo blog_urls.jsonl  
images/blog_urls/     # Imagens
scraped_blog_urls/    # ConteÃºdo HTML
```

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### Pipeline Customizado de Imagens

O projeto inclui um pipeline customizado que organiza imagens por nome do arquivo JSON:

```python
# settings.py
ITEM_PIPELINES = {
    'tutorial.pipelines.CustomImagesPipeline': 1,
}

# ConfiguraÃ§Ãµes de imagem
IMAGES_STORE = 'images'
IMAGES_MIN_HEIGHT = 50
IMAGES_MIN_WIDTH = 50
IMAGES_EXPIRES = 90  # dias
```

### Estrutura de OrganizaÃ§Ã£o

- **Por arquivo JSON**: Cada execuÃ§Ã£o cria pastas baseadas no nome do arquivo
- **Imagens separadas**: `images/[nome_arquivo]/`
- **ConteÃºdo separado**: `scraped_[nome_arquivo]/`
- **Nomes Ãºnicos**: Imagens com hash SHA1 da URL

### Rate Limiting e Delays

```python
# settings.py
DOWNLOAD_DELAY = 3
RANDOMIZE_DOWNLOAD_DELAY = True
AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_START_DELAY = 1
AUTOTHROTTLE_MAX_DELAY = 60
AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
```

### Headers Customizados

```python
# settings.py
DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'User-Agent': 'Mozilla/5.0 (compatible; MyBot/1.0)',
}
```

## ğŸ’¡ Exemplos de Uso

### Workflow Completo

```bash
# 1. Descobrir URLs com Katana
katana -u https://blog.exemplo.com -d 2 -o blog_urls.jsonl

# 2. Executar spider Scrapy
scrapy crawl page -a katana_file="blog_urls.jsonl"

# 3. Verificar resultados
ls scraped_blog_urls/
ls images/full/

# 4. AnÃ¡lise dos dados
find scraped_blog_urls/ -name "*.html" | wc -l
du -sh images/
```

### Scraping de E-commerce

```bash
# Descobrir pÃ¡ginas de produtos
katana -u https://loja.com -f qurl -ef css,js -o produtos.jsonl

# Scraping focado em imagens
scrapy crawl page -a katana_file="produtos.jsonl" -s IMAGES_MIN_HEIGHT=200
```

### Monitoramento de Site

```bash
# Descoberta diÃ¡ria
katana -u https://noticias.com -d 1 -o noticias_$(date +%Y%m%d).jsonl

# Scraping automatizado
scrapy crawl page -a katana_file="noticias_$(date +%Y%m%d).jsonl"
```

## ğŸš¨ Troubleshooting

### Problemas Comuns

#### 1. Erro: "Multiple spiders found"
```bash
# SoluÃ§Ã£o: Especificar o mÃ³dulo do spider
scrapy crawl page -a katana_file="urls.jsonl" -s SPIDER_MODULES=tutorial.spiders.crawlocal
```

#### 2. Erro: "ImagesPipeline requires Pillow"
```bash
# SoluÃ§Ã£o: Instalar/atualizar Pillow
pip install --upgrade Pillow
```

#### 3. Arquivo Katana vazio
```bash
# Verificar formato do arquivo
head -3 urls.jsonl
# Se vazio, regenerar com Katana
katana -u https://site.com -o urls.jsonl
```

#### 4. Muitas requisiÃ§Ãµes/Rate limiting
```bash
# Adicionar delay
scrapy crawl page -a katana_file="urls.jsonl" -s DOWNLOAD_DELAY=5
```

### Logs e Debug

```bash
# Debug completo
scrapy crawl page -a katana_file="urls.jsonl" -L DEBUG

# Salvar logs
scrapy crawl page -a katana_file="urls.jsonl" -L INFO -s LOG_FILE=debug.log

# Verificar estatÃ­sticas
grep -E "(item_scraped_count|file_count|elapsed_time)" debug.log
```

## ğŸ“Š Formatos Suportados

### Formato Katana Original
```json
{
  "timestamp": "2025-09-03T02:30:00Z",
  "request": {
    "method": "GET",
    "endpoint": "https://exemplo.com/pagina"
  },
  "response": {
    "status_code": 200
  }
}
```

### Formato Simplificado
```json
{
  "url": "https://exemplo.com/pagina"
}
```

## ğŸ† CaracterÃ­sticas

- âœ… **Dual Format**: Suporta formato Katana e simplificado
- âœ… **Auto Organization**: Pastas nomeadas automaticamente por arquivo JSON
- âœ… **Separated Images**: Imagens organizadas por origem do arquivo
- âœ… **Separated Content**: ConteÃºdo HTML organizado por origem do arquivo
- âœ… **Image Pipeline**: Download automÃ¡tico de imagens com detecÃ§Ã£o de extensÃ£o
- âœ… **Content Processing**: HTML original + formatado
- âœ… **URL Filtering**: Remove duplicatas e URLs invÃ¡lidas
- âœ… **Hash Naming**: Nomes Ãºnicos baseados em hash SHA1
- âœ… **Error Handling**: Tratamento robusto de erros
- âœ… **Scalable**: Processa milhares de URLs
- âœ… **Configurable**: Altamente customizÃ¡vel

## ğŸ“ LicenÃ§a

Este projeto Ã© fornecido como estÃ¡, para fins educacionais e de pesquisa. Respeite sempre os robots.txt e polÃ­ticas dos sites.

